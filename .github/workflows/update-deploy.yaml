name: Deploy and Benchmark Gaudi

on:
  push:
    branches:
      - main
  pull_request:
    branches:
      - main
  workflow_dispatch:

jobs:
  deploy-and-benchmark:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        cores: [16, 32]
        memory: [8000Mi, 16000Mi]
        model: ["meta-llama/Meta-Llama-3-8B-Instruct", "meta-llama/Meta-Llama-7B-Instruct"]

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Set Up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'

      - name: Install Dependencies
        run: |
          pip install -r requirements.txt || true

      - name: Generate Deployment YAML
        run: |
          cat <<EOF > deploy_config.yaml
          deploy:
            device: gaudi
            version: 1.2.0
            modelUseHostPath: /mnt/models
            HUGGINGFACEHUB_API_TOKEN: "${{ secrets.HUGGINGFACEHUB_API_TOKEN }}"
            node: [1, 2, 4, 8]
            namespace: ""
            timeout: 1000
            interval: 5
            services:
              llm:
                engine: vllm
                model_id: "${{ matrix.model }}"
                resources:
                  enabled: True
                  cores_per_instance: "${{ matrix.cores }}"
                  memory_capacity: "${{ matrix.memory }}"
                replicaCount:
                  with_teirerank: [7, 15, 31, 63]
                  without_teirerank: [8, 16, 32, 64]
          EOF

      - name: Deploy Services
        run: |
          ./deploy.sh deploy_config.yaml

      - name: Run Benchmark Tests
        run: |
          ./benchmark.sh deploy_config.yaml

      - name: Upload Benchmark Results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results
          path: results/*.json
